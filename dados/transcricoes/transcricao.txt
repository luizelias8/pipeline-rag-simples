Nesse vídeo, vamos explorar um modelo diferente, modelo capaz de transcrever áudio, estou falando do Whisper. O Whisper é um modelo open source mantido pela OpenAI, que também está disponível para ser consumido via API. Esse modelo multilingüe, além de fazer transcrição, também consegue traduzir outros idiomas para inglês. Para esse vídeo, eu pensei em um exemplo bem interessante, um sumarizador de vídeos do YouTube usando o Whisper e o LangChain. Como já vimos alguns recursos do LangChain em outros vídeos, como Chains, Prompts e Suponho que você está familiarizado. Vamos dar uma olhada no design do projeto para entender o que faremos nesse exemplo. Primeiro, baixamos o vídeo do YouTube, de nosso interesse, e o transcrevemos usando o Whisper. Em seguida, vamos criar resumos usando duas abordagens diferentes. Primeiro, utilizando o LangChain, sumarização existente, para gerar o resumo final, que trata automaticamente emberes e prompts. Depois, usamos outra abordagem mais detalhada para gerar um resumo final formatado, em tópicos ou bullet points. O plano é dividir a transcrição em chunks, gerar emberes e preparar prompts. O próximo passo é adicionar as chaves de API para os serviços da OpenAI Quadrant nas variáveis de ambiente, e eu já fiz isso, então vou carregar. Aqui nós temos uma função que vai baixar o arquivo MP4 do YouTube na melhor qualidade e vai salvar o arquivo localmente. Essa função recebe como argumento uma URL, essa é a versão mais rápida de implementar. Ela é baseada nessa biblioteca, YouTube DLP, que facilita bastante e trata todas as abstrações na hora de fazer o download de arquivos de vídeo ou transformar em arquivos de áudio. Para esse experimento, eu vou pegar um vídeo aqui do canal, você pode escolher qualquer outro, se preferir. Nesse vídeo eu falo um pouco sobre Word Embeddings e busco possibilidade. Vamos testar, vai demorar um tempinho, então eu volto assim que concluir. E funciona! Se a gente der uma olhada aqui na raiz do projeto, temos o arquivo InfoSlack MP4. Então, só para conferir, MP4, 271 megabytes. Ok, aqui eu tenho uma outra versão refatorada para extrair apenas o áudio, que é o que importa para o Whisper. Nos parâmetros eu escolho a qualidade do áudio, que vai ser extraído, bem como o codec, nesse caso MP3. Vamos testar outra vez, vou executar, e funciona muito bem. Comparando com o MP4, o MP3 é bem menor, então facilita na hora de trabalhar. Então vamos usar esse arquivo de MP3. Agora é a vez do Whisper, como eu falei no início, o Whisper é um sistema de reconhecimento automático de áudio bastante avançado, desenvolvido pela OpenAI. Esse modelo foi treinado em impressionantes 680 mil horas de dados supervisionados, multilingues e multitarefas, extraídos da web. Esse conjunto de dados vasto e variado aumenta bastante a capacidade do sistema, permitindo que ele lide facilmente com sotaques, ruídos de fundo, linguagem técnica. O pacote Whisper, que eu tenho instalado localmente, fornece o método LoadModel para baixar o modelo utilizado para transcrever um arquivo de vídeo ou áudio. Diversos tipos de modelos diferentes estão disponíveis, se a gente der uma conferida na documentação, nós temos o Tiny, Base, Small, Medium e Large. Cada um deles tem compensações entre precisão e velocidade. Vamos utilizar o modelo Base para esse tutorial, porque o poder de processamento dessa máquina que eu estou usando aqui, para gravar esse vídeo, não é tão grande, mas se você tiver uma GPU com memória suficiente para essa tarefa, a operação vai ficar bem mais rápida. Eu vou mostrar como utilizar o mesmo modelo pela API, que é o que provavelmente você vai utilizar em produção. Ok, antes de executar, eu vou botar a função especial Time para a gente comparar o tempo com a execução das APIs, só para ter o registro da execução local. Vai levar um tempo para concluir essa tarefa e eu volto quando finalizar. Ok, localmente levou 1 minuto e 8 segundos, que eu posso dar uma olhada no resultado da transcrição, então eu vou imprimir o Result e apenas o texto. Como é possível ver, com esse modelo Base utilizado, a transcrição não fica tão boa, ele erra algumas palavras. O que era para começar com Word Embeddings, são a forma de representar palavras e textos como vetores numéricos. Ele transcreveu como Word Embeddings, ficou bem estranho. A acurácia na transcrição do modelo Base não é tão boa para português. Vou fazer a mesma coisa utilizando a API da OpenAI, então eu vou passar o arquivo de áudio, vou fazer o upload desse arquivo de áudio de 10 MB para a API e fazer a transcrição com o modelo WhisperOne, seguindo as instruções da documentação da OpenAI. Aqui eu não estou utilizando o linkchain, é uma requisição direta na API da OpenAI. Em comparação com a execução local, pela API da OpenAI, levou 41 segundos versus 1 minuto e 8 segundos, é um pouco mais rápido. Vou salvar o conteúdo da transcrição em um arquivo de texto, chamado text.txt, aqui nós podemos dar uma conferida no resultado e comparar a qualidade da transcrição via API com o modelo que eu utilizei localmente, e a qualidade da transcrição via API já foi bem melhor, porque o modelo utilizado é um pouco mais robusto na API. Já começa com a palavra correta, Word Embeddings. Pensando em velocidade, a gente também pode fazer um experimento com o Grok, na infraestrutura do Grok ele suporta, via API, o modelo Whisper da OpenAI, usando a versão dele mais robusta, com poder de transcrição de maior qualidade. Então vou fazer a mesma coisa, utilizando o Grok, vou coletar o tempo e a gente compara com a API direta da OpenAI e o local. Em comparação com a API da OpenAI, pelo Grok, pela infraestrutura do Grok, levou 11 segundos. No local, 1 minuto e 8 segundos, via OpenAI, 41 segundos, via Grok, 11 segundos. Ok, continuando, agora vamos importar os componentes essenciais da biblioteca Langsham para a sumarização de texto e inicializar uma instância do LLM da OpenAI, no caso eu vou utilizar o chat OpenAI com o GPT-4 ou Mini, a versão mais recente, com a temperatura em 0. Os elementos-chave incluem classes para lidar com textos grandes, otimização, construção de prompts e técnicas de sumarização. Aqui eu estou configurando o splitter também, com chunks de 1000 caracteres, sem sobreposição, e utilizando espaços, vírgulas e quebras de linha como separadores, isso é para garantir que o texto de input seja dividido por esses separadores em vez de quebrar uma palavra ao meio. Com isso comprado, eu vou abrir o arquivo de texto, que foi salvo anteriormente, com a transcrição, e dividir, separar todo o conteúdo por meio do método split text, onde cada objeto document do LLM é inicializado com um conteúdo de um chunk da lista de textos. Então, se eu der uma conferida no índice 0 desse docs, eu tenho um chunk, ou a representação de um chunk. De toda a transcrição, capturei para exibir apenas um chunk. Agora vamos à sumarização. A abordagem stuff é a mais simples, na qual todo o texto do vídeo transcrito é usado em um único prompt. Esse método pode gerar exceções se todo o texto que queremos resumir for mais longo do que o tamanho do contexto disponível no LLM, e pode não ser a maneira mais eficiente de lidar com grandes quantidades de texto. Por coincidência, o modelo que nós escolhemos, GPT-4 ou mini, tem 128 mil tokens de contexto, então é espaço suficiente para fazer esse experimento com essa transcrição. Ok, vamos experimentar com o prompt template. Esse prompt vai gerar o resumo em forma de tópicos. Então eu tenho o prompt template, escreva um resumo conciso com bullet points do texto abaixo. Aqui eu vou inputar o conteúdo da transcrição como contexto, e o output aqui do meu prompt, que eu espero que o LLM resolva, é o resumo conciso que eu estou pedindo. E agora, vou inicializar a chain de sumarização, usando o tipo aqui, chain type stuff, como tipo de chain, e o prompt template que foi gerado. Então eu vou executar, ok, temos aqui a sumarização de toda a transcrição do vídeo. Lembrando que no linkchain temos a flexibilidade de criar prompts personalizados de acordo com as necessidades específicas. Por exemplo, se você deseja o resultado do resumo do vídeo em inglês, você pode implementar um prompt que instrua o modelo a gerar um resumo no idioma desejado. Nesse caso é bem melhor utilizar o LLM do que utilizar a função de translate dentro do próprio Whisper. O risco de alucinação na transcrição ou no translate do modelo Whisper é bem superior do que tentar traduzir isso aqui para inglês utilizando o GPT-4, por exemplo. Tudo o que fizemos até aqui foi seguir esse fluxo e obter essa sumarização final. Agora vamos trabalhar na segunda parte, que é seguir esse fluxo, gerar os chunks, transformar em embeddings, passar por um Vector Database, gerar um prompt e fazer um RAG. Vamos adicionar as transcrições no Vector Database, eu vou indexar a transcrição no quadrante e por meio de uma chain customizada faremos perguntas sobre o conteúdo da transcrição. Então aqui eu tenho o modelo de embeddings, textEmbeddingTreeSmall, da OpenAI, as variáveis de ambiente com o endpoint do meu cluster quadrante no cloud, mais a API Key, vou criar uma collection chamada Whisper e enviar os documentos da transcrição, os chunks. Vou executar, posso conferir aqui, uma nova collection foi criada chamada Whisper e nela contém os chunks da nossa transcrição. Então eu posso fazer um teste de similaridade, vou perguntar o que são word embeddings, fazer uma consulta no quadrante e com o output eu recebo 4 chunks, mais similares ao que foi perguntado. Agora eu vou continuar, utilizando o GPT-4 ou Mini em um RAG, com um prompt um pouco mais elaborado, vamos testar as perguntas e respostas. Então vou carregar as dependências, esse é o template do prompt customizado, utilize as transcrições abaixo para responder a pergunta em formato de bullet points e de forma resumida. Se não souber a resposta, diga apenas que não sabe, não tente inventar ou gerar uma resposta. Aqui tem a interpolação para passar o contexto como parâmetro, ou os chunks que eu vou receber, da minha busca por similaridade do Vector Database, a pergunta original do usuário, a query ou a consulta que eu vou enviar para esse sistema RAG, Resuma, Word Embeddings. Ele vai fazer a busca por similaridade, retornar os 3 chunks mais similares, carregar apenas o conteúdo de texto desses chunks e gerar uma resposta. Ok, como resposta temos Word Embeddings, representam palavras e textos como vetores numéricos e assim por diante. Bom, por hoje é só, espero que você tenha gostado, os links estão na descrição, muito obrigado por assistir e nos vemos no próximo.
